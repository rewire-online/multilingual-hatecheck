{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST CASE GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGES = [\"polish\"] #[\"french\", \"spanish\", \"portuguese\", \"hindi\", \"arabic\", \"mandarin\", \"italian\", \"polish\", \"german\", \"dutch\"]\n",
    "STORE_PATH = \"..\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Dictionary of Placeholder Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD FILES TO DATAFRAMES\n",
    "\n",
    "import_dict = dict()\n",
    "\n",
    "for lang in LANGUAGES:\n",
    "    import_dict[lang] = dict()\n",
    "\n",
    "    for filename in sorted(os.listdir(f\"{STORE_PATH}/2_Placeholders/{lang}/\")):\n",
    "        \n",
    "            if filename.endswith(\".csv\"):\n",
    "                \n",
    "                # need to handle case-inflected languages separately --> different file structure\n",
    "                if lang in [\"german\", \"polish\"]:\n",
    "                    import_dict[lang][filename.removesuffix(\".csv\")] = pd.read_csv(f\"{STORE_PATH}/2_Placeholders/{lang}/{filename}\")\n",
    "                    \n",
    "                else:\n",
    "                    import_df = pd.read_csv(f\"{STORE_PATH}/2_Placeholders/{lang}/{filename}\")\n",
    "                    \n",
    "                    for col in import_df.columns:\n",
    "                        if col!=\"TARGET\":\n",
    "                            import_dict[lang][col] = import_df[[\"TARGET\", col]]\n",
    "                            import_dict[lang][col].columns = [\"TARGET\", \"PLACEHOLDER\"]\n",
    "                            \n",
    "                            # for gender-inflected languages, we need to delete rows with empty entries\n",
    "                            import_dict[lang][col] = import_dict[lang][col].drop(import_dict[lang][col][import_dict[lang][col].PLACEHOLDER==\"-\"].index)\n",
    "                            import_dict[lang][col].dropna(inplace=True)\n",
    "                            \n",
    "            else:\n",
    "                print(f\"unexpected file: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "polish dict_keys(['[IDENT_A]', '[IDENT_P]', '[IDENT_P_char_del]', '[IDENT_P_leet]', '[IDENT_P_space_add]', '[IDENT_S]', '[IDENT_S_char_del]', '[IDENT_S_leet]', '[IDENT_S_space_add]', '[SLR_P]', '[SLR_P_leet]', '[SLR_P_space_add]', '[SLR_S]', '[SLR_S_leet]', '[SLR_S_space_add]', '[female_IDENT_P]', '[female_IDENT_P_char_del]', '[female_IDENT_P_leet]', '[female_IDENT_P_space_add]', '[female_IDENT_S]', '[female_IDENT_S_char_del]', '[female_IDENT_S_leet]', '[female_IDENT_S_space_add]', '[female_SLR_P]', '[female_SLR_P_leet]', '[female_SLR_P_space_add]', '[female_SLR_S]', '[female_SLR_S_leet]', '[female_SLR_S_space_add]', '[male_IDENT_P]', '[male_IDENT_P_char_del]', '[male_IDENT_P_leet]', '[male_IDENT_P_space_add]', '[male_IDENT_S]', '[male_IDENT_S_char_del]', '[male_IDENT_S_leet]', '[male_IDENT_S_space_add]', '[male_SLR_P]', '[male_SLR_P_leet]', '[male_SLR_P_space_add]', '[male_SLR_S]', '[male_SLR_S_leet]', '[male_SLR_S_space_add]']) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COLLAPSE COLUMNS IN DATAFRAMES INTO LISTS OF STRINGS IN PLACEHOLDER DICT\n",
    "\n",
    "placeholder_dict = {}\n",
    "\n",
    "for lang in LANGUAGES:\n",
    "    placeholder_dict[lang] = dict()\n",
    "\n",
    "    for df in import_dict[lang]:\n",
    "        placeholder_dict[lang][df] = {}\n",
    "        for column in import_dict[lang][df].columns:\n",
    "            placeholder_dict[lang][df][column] = import_dict[lang][df][column].to_list()\n",
    "    print(lang, placeholder_dict[lang].keys(),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Template Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "polish (755, 8)\n"
     ]
    }
   ],
   "source": [
    "cases_dict = dict()\n",
    "\n",
    "for lang in LANGUAGES:\n",
    "    \n",
    "    # import data from csv file\n",
    "    cases_dict[lang] = pd.read_csv(f\"{STORE_PATH}/1_Templates/hatecheck_templates_{lang}.csv\")\n",
    "\n",
    "    # drop unneccessary columns\n",
    "    cases_dict[lang].drop(columns = [\"focus_lemma\", \"trans_deepl\", \"trans_google\", \"case_templ\", \"number\"], inplace = True, errors = \"ignore\")\n",
    "\n",
    "    # rename manual translation column to be new case_templ column\n",
    "    cases_dict[lang].rename(columns = {\"trans_manual\": \"case_templ\"}, inplace = True)\n",
    "    \n",
    "    # create label_gold column if it does not exist already\n",
    "    if \"label_gold\" not in cases_dict[lang].columns:\n",
    "        cases_dict[lang][\"label_gold\"] = cases_dict[lang].apply(lambda x: \"hateful\" if x.functionality.endswith(\"_h\") else \"non-hateful\", axis = 1)\n",
    "    \n",
    "    # tidy up column types\n",
    "    cases_dict[lang] = cases_dict[lang].convert_dtypes()\n",
    "    cases_dict[lang]['ref_templ_id'] = cases_dict[lang].ref_templ_id.astype('Int64')\n",
    "    \n",
    "    print(lang, cases_dict[lang].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explode Templates into Individual Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function for writing lists of test cases and target identities from templates\n",
    "\n",
    "def write_case_target(row, lang):\n",
    "    test_case=[]\n",
    "    target_ident=[] \n",
    "    \n",
    "    # for gender-inflected templates, look at gender-specific columns\n",
    "    if pd.isnull(row.case_templ):\n",
    "        for key in placeholder_dict[lang]:\n",
    "            if key in row.gender_male:\n",
    "                \n",
    "                if lang in [\"polish\", \"german\"]:\n",
    "                    for item in placeholder_dict[lang][key][row.case]:\n",
    "                        test_case.append(row.gender_male.replace(key, item))\n",
    "                else:\n",
    "                    for item in placeholder_dict[lang][key][\"PLACEHOLDER\"]:\n",
    "                        test_case.append(row.gender_male.replace(key, item))\n",
    "                        \n",
    "                for item in placeholder_dict[lang][key][\"TARGET\"]:\n",
    "                    target_ident.append(item)\n",
    "            \n",
    "            if key in row.gender_female:\n",
    "                if lang in [\"polish\", \"german\"]:\n",
    "                    for item in placeholder_dict[lang][key][row.case]:\n",
    "                        test_case.append(row.gender_female.replace(key, item))\n",
    "                else:\n",
    "                    for item in placeholder_dict[lang][key][\"PLACEHOLDER\"]:\n",
    "                        test_case.append(row.gender_female.replace(key, item))\n",
    "                        \n",
    "                for item in placeholder_dict[lang][key][\"TARGET\"]:\n",
    "                    target_ident.append(item)\n",
    "        return test_case, target_ident\n",
    "    \n",
    "    # for not gender-inflected templates, use standard column      \n",
    "    for key in placeholder_dict[lang]:\n",
    "        if key in row.case_templ:\n",
    "            \n",
    "            if lang in [\"polish\", \"german\"]:\n",
    "                for item in placeholder_dict[lang][key][row.case]:\n",
    "                    test_case.append(row.case_templ.replace(key, item))\n",
    "            else:\n",
    "                for item in placeholder_dict[lang][key][\"PLACEHOLDER\"]:\n",
    "                    test_case.append(row.case_templ.replace(key, item))\n",
    "                    \n",
    "            for item in placeholder_dict[lang][key][\"TARGET\"]:\n",
    "                target_ident.append(item)\n",
    "    \n",
    "    # for templates without placeholders, the case is just the template itself\n",
    "    if test_case==[]:\n",
    "        test_case = row.case_templ\n",
    "    return test_case, target_ident"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "polish (3815, 10)\n"
     ]
    }
   ],
   "source": [
    "# write lists of test cases from templates\n",
    "\n",
    "for lang in LANGUAGES:\n",
    "\n",
    "    cases_dict[lang]['test_case'] = cases_dict[lang].apply(lambda x: write_case_target(x, lang), axis=1)\n",
    "    cases_dict[lang]['target_ident'] = cases_dict[lang].test_case.apply(lambda x: x[1])\n",
    "    cases_dict[lang]['test_case'] = cases_dict[lang].test_case.apply(lambda x: x[0])\n",
    "    \n",
    "    # explode templates \n",
    "    cases_dict[lang] = pd.concat([cases_dict[lang].explode('test_case').drop(columns=['target_ident']),\n",
    "                                  pd.Series.explode(cases_dict[lang].target_ident)], axis=1)\n",
    "    \n",
    "    # tidy up column types again\n",
    "    cases_dict[lang] = cases_dict[lang].convert_dtypes()\n",
    "    \n",
    "    print(lang, cases_dict[lang].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create References Between Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.28 s, sys: 5.43 ms, total: 1.28 s\n",
      "Wall time: 1.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def case_id_finder(row, df):\n",
    "    if pd.notna(row.ref_templ_id) and pd.notna(row.target_ident):\n",
    "        output = df.case_id[(df.templ_id==row.ref_templ_id)&(df.target_ident==row.target_ident)].values\n",
    "        if len(output)<2 and len(output)>0:\n",
    "            return output[0]\n",
    "    else:\n",
    "        return pd.NA\n",
    "\n",
    "for lang in LANGUAGES:\n",
    "\n",
    "    # create case IDs for every case\n",
    "    cases_dict[lang].reset_index(inplace=True)\n",
    "    cases_dict[lang]['case_id']=cases_dict[lang].index + 1\n",
    "    \n",
    "    # Match ref_templ_ids to case IDs. Only works for identity terms, not slurs\n",
    "    cases_dict[lang]['ref_case_id'] = cases_dict[lang].apply(lambda x: case_id_finder(x, cases_dict[lang]), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language-specific Tidying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capitalise first letter of all test cases in Polish, Dutch, Italian, Portuguese, Spanish, French (needed because of lowercase placeholders)\n",
    "for lang in [\"polish\", \"dutch\", \"italian\", \"portuguese\", \"spanish\", \"french\"]:\n",
    "    if lang in LANGUAGES:\n",
    "        cases_dict[lang][\"test_case\"] = cases_dict[lang].test_case.apply(lambda s: s[0].upper() + s[1:])\n",
    "        \n",
    "# For Italian, adjust articles based on first letter(s) of noun --> only relevant IDENT is immigrati\n",
    "def italian_articles(test_case, letters):\n",
    "    for l in letters:\n",
    "        test_case = test_case.replace(f\" i {l}\",f\" gli {l}\")\n",
    "        test_case = test_case.replace(f\" ai {l}\",f\" agli {l}\")\n",
    "        test_case = test_case.replace(f\" il {l}\",f\" l'{l}\")\n",
    "        test_case = test_case.replace(f\"I {l}\",f\"Gli {l}\")\n",
    "        test_case = test_case.replace(f\"Ai {l}\",f\"Agli {l}\")\n",
    "        test_case = test_case.replace(f\"Il {l}\",f\"L'{l}\")\n",
    "        test_case = test_case.replace(f\" sui {l}\",f\" sugli {l}\")\n",
    "        test_case = test_case.replace(f\" quel {l}\",f\" quell'{l}\")\n",
    "        test_case = test_case.replace(f\" quei {l}\",f\" quegli {l}\")\n",
    "        test_case = test_case.replace(f\"Sui {l}\",f\"Sugli {l}\")\n",
    "        test_case = test_case.replace(f\"Quel {l}\",f\"Quell'{l}\")\n",
    "        test_case = test_case.replace(f\"Quei {l}\",f\"Quegli {l}\")\n",
    "    return test_case\n",
    "\n",
    "if \"italian\" in LANGUAGES:\n",
    "    for rel in [\"immigr\", \"immgra\", \"i m m i g r\", \"imm1g\"]:\n",
    "        cases_dict[\"italian\"][\"test_case\"] = cases_dict[\"italian\"].test_case.apply(lambda x: italian_articles(x, [rel]))\n",
    "\n",
    "    \n",
    "# For French, adjust articles based on first letter(s) of noun --> relevant IDENT are homo and handicape, SLR attarde mental\n",
    "def french_articles(test_case, letters):\n",
    "    for l in letters:\n",
    "        test_case = test_case.replace(f\" le {l}\",f\" l'{l}\")\n",
    "        test_case = test_case.replace(f\" la {l}\",f\" l'{l}\")\n",
    "        test_case = test_case.replace(f\" de {l}\",f\" d'{l}\")\n",
    "        test_case = test_case.replace(f\" ce {l}\",f\" cet{l}\")\n",
    "        test_case = test_case.replace(f\" quelque {l}\",f\" quelqu'{l}\")\n",
    "        test_case = test_case.replace(f\"Le {l}\",f\"L'{l}\")\n",
    "        test_case = test_case.replace(f\"La {l}\",f\"L'{l}\")\n",
    "        test_case = test_case.replace(f\"De {l}\",f\"D'{l}\")\n",
    "        test_case = test_case.replace(f\"Ce {l}\",f\"Cet{l}\")\n",
    "        test_case = test_case.replace(f\"Quelque {l}\",f\"Quelqu'{l}\")\n",
    "    return test_case\n",
    "\n",
    "if \"french\" in LANGUAGES:\n",
    "    for rel in [\"homo\", \"hoom\", \"omo\", \"h o m\", \"h0mo\",\n",
    "                \"handic\", \"handci\", \"h a n d\", \"h4ndi\",\n",
    "                \"attard\", \"a t t a r\", \"attrd\", \"att4rd\", \"attrad\"]:\n",
    "        cases_dict[\"french\"][\"test_case\"] = cases_dict[\"french\"].test_case.apply(lambda x: french_articles(x, [rel]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if any test cases are NA\n",
    "for lang in LANGUAGES:\n",
    "    if cases_dict[lang].test_case.isnull().sum()>0:\n",
    "        print(lang.upper())\n",
    "        display(cases_dict[lang][cases_dict[lang].test_case.isnull()])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if any test cases still contain IDENT or SLR placeholders\n",
    "for lang in LANGUAGES:\n",
    "    for ph in [\"IDENT\", \"SLR\", \"_\"]:\n",
    "        if cases_dict[lang].test_case.str.contains(ph).sum()>0:\n",
    "            print(lang.upper(), ph)\n",
    "            display(cases_dict[lang][cases_dict[lang].test_case.str.contains(ph)])\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if any gender-inflected templates contain non-inflected placeholders\n",
    "for lang in LANGUAGES:\n",
    "    if \"gender_male\" in cases_dict[lang].columns:\n",
    "        for ph in [\"\\[IDENT\", \"\\[SLR\"]:\n",
    "            if cases_dict[lang].gender_male.str.contains(ph).sum()>0:\n",
    "                print(lang.upper(), \"MALE\")\n",
    "                display(cases_dict[lang][cases_dict[lang].gender_male.str.contains(ph)])\n",
    "                print()\n",
    "            if cases_dict[lang].gender_female.str.contains(ph).sum()>0:\n",
    "                print(lang.upper(), \"FEMALE\")\n",
    "                display(cases_dict[lang][cases_dict[lang].gender_female.str.contains(ph)])\n",
    "                print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "لازم نعلق كلللللللل سود دول من رقابهم. \n",
      "\n",
      "دا مناسب لاللاجئين الحشرات الي زييييييك. \n",
      "\n",
      "هي دي العاهره على حقيقتها. \n",
      "\n",
      "مش لازم نخلي المعاقيين يعانو اكتر من مهما اصلا. \n",
      "\n",
      "ما فيش 7اجة اسواء من اليهودي. \n",
      "\n",
      "احنا مجموعة من اليهود. \n",
      "\n",
      "المعاقيين زبالة. \n",
      "\n",
      "اليهود هم بس اللي جاهلين للدرجة دي. \n",
      "\n",
      "هقص رقاب المعاقيين. \n",
      "\n",
      "اغتصاب النساء الي متلك لازم يبقى حلال. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "lang = \"arabic\"\n",
    "\n",
    "for i in cases_dict[lang].test_case.sample(10):\n",
    "    print(i,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in LANGUAGES:\n",
    "    \n",
    "    cases_dict[lang][\"mhc_case_id\"] = cases_dict[lang].case_id.apply(lambda x: lang + \"-\"+ str(x))\n",
    "    \n",
    "    if \"gender_male\" in cases_dict[lang].columns:\n",
    "        export_df = cases_dict[lang][['functionality', \"mhc_case_id\", 'case_id', 'test_case', 'label_gold', 'target_ident', 'ref_case_id',\n",
    "                                    'ref_templ_id', 'templ_id','case_templ', 'gender_male', 'gender_female']].copy()\n",
    "\n",
    "    else:\n",
    "        export_df = cases_dict[lang][['functionality', \"mhc_case_id\", 'case_id', 'test_case', 'label_gold', 'target_ident', 'ref_case_id',\n",
    "                            'ref_templ_id', 'templ_id','case_templ']].copy()\n",
    "\n",
    "    \n",
    "    export_df.to_csv(f\"{STORE_PATH}/3_Generated Cases/hatecheck_cases_{lang}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in LANGUAGES:\n",
    "\n",
    "    export_df = cases_dict[lang][[\"mhc_case_id\", 'test_case']].copy()\n",
    "    export_df = export_df.sample(frac = 1,random_state=123)\n",
    "\n",
    "    export_df.to_csv(f\"../4_Cases for Annotation/hatecheck_cases_for_annotation_{lang}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
